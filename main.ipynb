{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847f372b",
   "metadata": {
    "id": "847f372b"
   },
   "source": [
    "# AI vs. Human-Generated Images Detection\n",
    "\n",
    "\n",
    "**Course**: INFO-6147  \n",
    "**Student**: Harrison Kim, 1340629  \n",
    "**Date**: August 15, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f18cce",
   "metadata": {
    "id": "92f18cce"
   },
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "021ffdca",
   "metadata": {
    "executionInfo": {
     "elapsed": 4194,
     "status": "ok",
     "timestamp": 1755306866077,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "021ffdca"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision kagglehub pandas tqdm matplotlib scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99bce07a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755306866085,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "99bce07a",
    "outputId": "636ba222-6d44-46e4-91c7-e4a106a04a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de7460",
   "metadata": {
    "id": "10de7460"
   },
   "source": [
    "## 1. Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e6e68",
   "metadata": {
    "id": "673e6e68"
   },
   "source": [
    "### 1.1 Choose a suitable image dataset for your project. You can consider any of the well-known datasets here Datasets â€” [Torchvision 0.17 documentation](https://docs.pytorch.org/vision/stable/datasets.html) for simplicity.\n",
    "\n",
    "- decided to use [*`CIFAKE: Real and AI-Generated Synthetic Images`*](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "955dd9ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1755306867924,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "955dd9ab",
    "outputId": "65cf5137-c007-4b83-ba44-49503543c961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dataset to: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n",
      "Number of rows: 60000\n",
      "DataFrame columns: Index(['image_path', 'label', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Check if dataset already exists\n",
    "expected_dir = os.path.expanduser(\"~/.cache/kagglehub/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/versions/3\")\n",
    "if not os.path.exists(expected_dir):\n",
    "    path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
    "    print(\"Downloaded dataset to:\", path)\n",
    "else:\n",
    "    path = expected_dir\n",
    "    print(\"Dataset already exists at:\", path)\n",
    "\n",
    "# Define image folders (usually 'train' and 'test' or 'real' and 'fake')\n",
    "image_root = os.path.join(path)\n",
    "folders = [\"train\", \"test\"]\n",
    "labels = [\"REAL\", \"FAKE\"]\n",
    "\n",
    "rows = []\n",
    "for split in folders:\n",
    "    for label in labels:\n",
    "        folder_path = os.path.join(image_root, split, label)\n",
    "        if os.path.exists(folder_path):\n",
    "            # for fname in os.listdir(folder_path):\n",
    "            #     if fname.lower().endswith(('.jpg')):\n",
    "            #         rows.append({\n",
    "            #             \"image_path\": os.path.join(folder_path, fname),\n",
    "            #             \"label\": label,\n",
    "            #             \"split\": split\n",
    "            #         })\n",
    "            file_list = [fname for fname in os.listdir(folder_path) if fname.lower().endswith('.jpg')]\n",
    "            half_len = len(file_list) // 2\n",
    "            for fname in file_list[:half_len]:\n",
    "                rows.append({\n",
    "                    \"image_path\": os.path.join(folder_path, fname),\n",
    "                    \"label\": label,\n",
    "                    \"split\": split\n",
    "                })\n",
    "\n",
    "# Check rows length and DataFrame columns\n",
    "print('Number of rows:', len(rows))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "print('DataFrame columns:', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd94a3e",
   "metadata": {
    "id": "1dd94a3e"
   },
   "source": [
    "### 1.2 Ensure that the dataset contains a reasonable number of classes and a sufficient number of images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c64bede",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755306867927,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "6c64bede",
    "outputId": "0d4d4ebb-c097-4bf9-8e35-00a9f0b02def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Label names: ['REAL' 'FAKE']\n"
     ]
    }
   ],
   "source": [
    "# Print the number of unique classes in the 'label' column\n",
    "num_classes = df['label'].nunique()\n",
    "print('Number of classes:', num_classes)\n",
    "print('Label names:', df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f93dd",
   "metadata": {
    "id": "9b8f93dd"
   },
   "source": [
    "### 1.3 If the dataset has very large number of images, you can use a subset (e.g., 1000 images per class if number of classes are 10 or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "719b60b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755306867929,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "719b60b6",
    "outputId": "6b456f27-b55c-41e2-cb5b-79c39edd763f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per class:\n",
      "label\n",
      "REAL    30000\n",
      "FAKE    30000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the number of images per class\n",
    "image_counts = df['label'].value_counts()\n",
    "print('Number of images per class:')\n",
    "print(image_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6195f8",
   "metadata": {
    "id": "9c6195f8"
   },
   "source": [
    "### 1.4 If the dataset has more than 20 classes, you can use a subset of the classes (e.g., only use 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de94e6e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755306867930,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "de94e6e2",
    "outputId": "88e68600-3746-4d81-f8c1-ae9e552af026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all classes as we only have 2 classes. ==> ['REAL' 'FAKE']\n"
     ]
    }
   ],
   "source": [
    "if num_classes > 20:\n",
    "    print(\"Using a subset of classes due to high number of classes.\")\n",
    "else:\n",
    "    print(f\"Using all classes as we only have {num_classes} classes. ==> {df['label'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a0094",
   "metadata": {
    "id": "b61a0094"
   },
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888e41f",
   "metadata": {
    "id": "7888e41f"
   },
   "source": [
    "### 2.1 Perform data preprocessing steps such as resizing images, normalizing pixel values, and splitting the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "139b137d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1755306868110,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "139b137d",
    "outputId": "e0002c4a-b1aa-48a8-9ca2-b6b4e31db8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 40000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define ImageDataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.image_paths = dataframe['image_path'].values\n",
    "        self.labels = dataframe['label'].values\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_idx = self.label_to_idx[label]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image {img_path}:', e)\n",
    "            img = torch.zeros(3, 32, 32)\n",
    "        return img, label_idx\n",
    "\n",
    "# 1. Split df into train, validation and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(\n",
    "    temp,\n",
    "    test_size=0.2,\n",
    "    stratify=temp['label'],\n",
    "    random_state=42\n",
    ")\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "# 2. Define transform (resize, normalize)\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# 3. Create ImageDataset for each split\n",
    "# train_dataset = ImageDataset(train_df, transform=transform)\n",
    "val_dataset = ImageDataset(val_df, transform=transform)\n",
    "test_dataset = ImageDataset(test_df, transform=transform)\n",
    "\n",
    "print('Train set size:', len(train_df))\n",
    "print('Validation set size:', len(val_dataset))\n",
    "print('Test set size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353229b7",
   "metadata": {
    "id": "353229b7"
   },
   "source": [
    "### 2.2 Apply data augmentation techniques to increase the diversity of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85d94642",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755306868110,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "85d94642",
    "outputId": "6965c0bd-ca10-40da-871d-bd9fa704198b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation applied to training dataset.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Re-create train_dataset with augmentation\n",
    "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
    "print('Data augmentation applied to training dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4a981",
   "metadata": {
    "id": "3ea4a981"
   },
   "source": [
    "## 3. Model Selection and Architecture\n",
    "\n",
    "- Select an appropriate deep learning architecture for image classification. You can start with a convolutional neural network (CNN).\n",
    "- Define the architecture of your model, including the number of layers, activation functions, and any regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7131ca3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755306868110,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "7131ca3d",
    "outputId": "7e8e242b-fdc8-4205-ba2b-61839c210ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model with dropout(0.5) initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = Net(num_classes=num_classes)\n",
    "\n",
    "print(\"CNN model with dropout(0.5) initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0c96e",
   "metadata": {
    "id": "23f0c96e"
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7vJVip8ZZrJn",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755306868110,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "7vJVip8ZZrJn"
   },
   "outputs": [],
   "source": [
    "# Initial hyperparameter values\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 25\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d4ad9",
   "metadata": {
    "id": "f27d4ad9"
   },
   "source": [
    "### 4.1 Train your deep learning model using the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a8f1f",
   "metadata": {
    "id": "a83a8f1f"
   },
   "source": [
    "#### 4.1.1 Load dataset into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa56864b",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755306868111,
     "user": {
      "displayName": "Harrison Kim",
      "userId": "05025015238796620338"
     },
     "user_tz": 240
    },
    "id": "fa56864b"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb38655",
   "metadata": {
    "id": "bbb38655"
   },
   "source": [
    "#### 4.1.2 Train custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e3627",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a47e3627",
    "outputId": "49d044b5-7683-4f2f-c81d-1e6ddf019cac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:52<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Train Loss: 0.5472 | Train Acc: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:46<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 | Train Loss: 0.4396 | Train Acc: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:46<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 | Train Loss: 0.3934 | Train Acc: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:45<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 | Train Loss: 0.3571 | Train Acc: 0.8455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:45<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 | Train Loss: 0.3304 | Train Acc: 0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:45<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 | Train Loss: 0.3103 | Train Acc: 0.8715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:46<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 | Train Loss: 0.2891 | Train Acc: 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:46<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 | Train Loss: 0.2765 | Train Acc: 0.8869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:45<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 | Train Loss: 0.2656 | Train Acc: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 496/625 [00:36<00:08, 14.44it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop for custom CNN model\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Lists to store metrics\n",
    "cnn_train_loss_list = []\n",
    "cnn_train_acc_list = []\n",
    "cnn_val_loss_list = []\n",
    "cnn_val_acc_list = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        images = images.to(device)\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Store metrics\n",
    "    cnn_train_loss_list.append(train_loss)\n",
    "    cnn_train_acc_list.append(train_acc)\n",
    "\n",
    "    # Calculate and record validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataset:\n",
    "            images = images.to(device)\n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    cnn_val_loss_list.append(val_loss)\n",
    "    cnn_val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        counter = 0\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= 5:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"cnn_train_loss_list:\", cnn_train_loss_list)\n",
    "print(\"cnn_train_acc_list:\", cnn_train_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752750c3",
   "metadata": {
    "id": "752750c3"
   },
   "source": [
    "### 4.2 Monitor training progress, including loss and accuracy, and consider using early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3e341",
   "metadata": {
    "id": "5cc3e341"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cnn_train_loss_list, label='Train Loss')\n",
    "plt.plot(cnn_val_loss_list, label='Val Loss')\n",
    "plt.title('Loss (Train vs Validation)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(cnn_train_acc_list, label='Train Acc')\n",
    "plt.plot(cnn_val_acc_list, label='Val Acc')\n",
    "plt.title('Accuracy (Train vs Validation)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac96b",
   "metadata": {
    "id": "964ac96b"
   },
   "source": [
    "## 5. Hyperparameter Tuning:\n",
    "\n",
    "- Experiment with different hyperparameters (e.g., learning rate, batch size) to optimize the model's performance.\n",
    "- Keep a record of the hyperparameters used and their impact on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9002fa",
   "metadata": {
    "id": "cf9002fa"
   },
   "source": [
    "### 5.1 Experiment with different hyperparameters (e.g., learning rate, batch size) to optimize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14a9a1",
   "metadata": {
    "id": "4a14a9a1"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter search for learning rate and batch size\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "search_learning_rates = [0.0005, 0.001, 0.005]\n",
    "search_batch_sizes = [128, 192, 256]\n",
    "num_epochs_hp = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in search_learning_rates:\n",
    "    for batch_size in search_batch_sizes:\n",
    "        print(f\"\\nTraining with learning rate={lr}, batch size={batch_size}\")\n",
    "        # Re-create dataloaders with new batch size\n",
    "        train_loader = DataLoader(train_dataset.dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset.dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Re-initialize model and optimizer\n",
    "        model = Net(num_classes=num_classes).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        counter = 0\n",
    "        for epoch in range(num_epochs_hp):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_hp} (train)\"):\n",
    "                images = images.to(device)\n",
    "                if not isinstance(labels, torch.Tensor):\n",
    "                    labels = torch.tensor(labels)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            train_loss = running_loss / total\n",
    "            train_acc = correct / total\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(val_dataset, desc=\"Validating\"):\n",
    "                    images = images.to(device)\n",
    "                    if not isinstance(labels, torch.Tensor):\n",
    "                        labels = torch.tensor(labels)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "            val_loss = val_loss / val_total\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= 3:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'best_val_acc': best_val_acc\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02fb5a",
   "metadata": {
    "id": "2b02fb5a"
   },
   "source": [
    "### 5.2 Keep a record of the hyperparameters used and their impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26031a99",
   "metadata": {
    "id": "26031a99"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to summarize hyperparameter search results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Hyperparameter summary table:\")\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, display the best configuration\n",
    "best_row = results_df.loc[results_df['best_val_acc'].idxmax()]\n",
    "print(f\"\\nBest configuration:\\nLearning rate: {best_row['learning_rate']}, Batch size: {best_row['batch_size']}, Best Val Acc: {best_row['best_val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c134d",
   "metadata": {
    "id": "a47c134d"
   },
   "source": [
    "## 6. Evaluation:\n",
    "\n",
    "- Evaluate your trained model using the validation dataset to assess its performance.\n",
    "- Calculate relevant metrics such as accuracy, precision, recall, and F1-score.\n",
    "- Visualize the model's predictions and misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6033d",
   "metadata": {
    "id": "bda6033d"
   },
   "source": [
    "### 6.1 Evaluate your trained model using the validation dataset to assess its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d534a",
   "metadata": {
    "id": "795d534a"
   },
   "outputs": [],
   "source": [
    "# Evaluate CNN model on validation dataset\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "cnn_true = []\n",
    "cnn_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_dataset, desc=\"Validating\"):\n",
    "        images = images.to(device)\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "        val_total += labels.size(0)\n",
    "        cnn_true.extend(labels.cpu().numpy())\n",
    "        cnn_pred.extend(predicted.cpu().numpy())\n",
    "val_loss = val_loss / val_total\n",
    "val_acc = val_correct / val_total\n",
    "print(f\"\\nCNN Validation Loss: {val_loss:.4f} | CNN Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bcacb",
   "metadata": {
    "id": "882bcacb"
   },
   "source": [
    "### 6.2 Calculate relevant metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b54e2",
   "metadata": {
    "id": "216b54e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# CNN metrics\n",
    "cnn_acc = accuracy_score(cnn_true, cnn_pred)\n",
    "cnn_prec = precision_score(cnn_true, cnn_pred, average='macro')\n",
    "cnn_rec = recall_score(cnn_true, cnn_pred, average='macro')\n",
    "cnn_f1 = f1_score(cnn_true, cnn_pred, average='macro')\n",
    "print('CNN metrics:')\n",
    "print(f'Accuracy: {cnn_acc:.4f}, Precision: {cnn_prec:.4f}, Recall: {cnn_rec:.4f}, F1-score: {cnn_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0f3ce",
   "metadata": {
    "id": "f8f0f3ce"
   },
   "source": [
    "## 8. Final Model Testing:\n",
    "\n",
    "- Test your final model on the held-out test dataset to assess its generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c2381",
   "metadata": {
    "id": "097c2381"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_true = []\n",
    "test_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataset:\n",
    "        images = images.to(device)\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_true.extend(labels.cpu().numpy())\n",
    "        test_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_true, test_pred)\n",
    "test_prec = precision_score(test_true, test_pred, average='macro')\n",
    "test_rec = recall_score(test_true, test_pred, average='macro')\n",
    "test_f1 = f1_score(test_true, test_pred, average='macro')\n",
    "print(f'Test Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1-score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e6646",
   "metadata": {
    "id": "f61e6646"
   },
   "source": [
    "## 9. Documentation and Reporting:\n",
    "\n",
    "- Create a project report summarizing your dataset, model architecture, training process, evaluation results, and insights gained.\n",
    "- Include visualizations and explanations to make your findings clear.\n",
    "\n",
    "## 10. Presentation:\n",
    "\n",
    "- Prepare a brief presentation to showcase your project's key findings and outcomes.\n",
    "- Share your experiences, challenges faced, and lessons learned during the project.\n",
    "\n",
    "## 11. Conclusion:\n",
    "\n",
    "- Conclude your capstone project by summarizing your achievements and any future work or improvements that could be made to the model.\n",
    "- Remember to maintain good coding practices and seek guidance or feedback from your instructor throughout the project.\n",
    "- This capstone project will demonstrate your ability to apply deep learning techniques to real-world problems and showcase your skills to potential employers or collaborators."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "10de7460",
    "b61a0094",
    "3ea4a981",
    "23f0c96e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "6147",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
